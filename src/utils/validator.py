"""
Validador de Respostas
Valida qualidade, confiabilidade e conformidade das respostas geradas.
"""
from __future__ import annotations

import re
from typing import List, Dict, Any, Optional, TYPE_CHECKING
from dataclasses import dataclass

if TYPE_CHECKING:
    # Apenas para type hints (n√£o executa em runtime)
    from src.services.answer_service import ConfidenceLevel, Evidence



@dataclass
class ValidationResult:
    """Resultado da valida√ß√£o"""
    is_valid: bool
    confidence: "ConfidenceLevel"
    warnings: List[str]
    scores: Dict[str, float]


class ResponseValidator:
    """
    Valida respostas do sistema RAG
    """
    
    def __init__(self):
        # Padr√µes de n√£o resposta
        self.no_answer_patterns = [
            r"n√£o localizado",
            r"n√£o foi encontrad[oa]",
            r"insuficiente",
            r"n√£o h√° informa√ß√£o",
            r"n√£o consta",
        ]
        
        # Padr√µes de conflito
        self.conflict_patterns = [
            r"conflito normativo",
            r"interpreta√ß√µes conflitantes",
            r"diverg√™ncia",
        ]
        
        # Marcadores de cita√ß√£o
        self.citation_pattern = r"\[(\d+)\]"
    
    def validate_response(
        self,
        question: str,
        answer: str,
        evidences: List["Evidence"],
        avg_score: float
    ) -> Dict[str, Any]:       
        from src.services.answer_service import ConfidenceLevel
        """
        Valida resposta completa e retorna n√≠vel de confian√ßa
        """
        from src.services.answer_service import ConfidenceLevel, Evidence
        warnings = []
        scores = {}
        
        # 1. Verifica se √© uma n√£o-resposta
        if self._is_no_answer(answer):
            return {
                "is_valid": True,
                "confidence": ConfidenceLevel.INSUFICIENTE,
                "warnings": ["Resposta indica informa√ß√£o insuficiente"],
                "scores": {}
            }
        
        # 2. Valida cita√ß√µes
        citation_score = self._validate_citations(answer, len(evidences))
        scores["citations"] = citation_score
        
        if citation_score < 0.5:
            warnings.append("Poucas cita√ß√µes encontradas na resposta")
        
        # 3. Valida completude
        completeness_score = self._validate_completeness(answer, question)
        scores["completeness"] = completeness_score
        
        if completeness_score < 0.5:
            warnings.append("Resposta pode estar incompleta")
        
        # 4. Valida qualidade das evid√™ncias
        evidence_score = self._validate_evidence_quality(evidences, avg_score)
        scores["evidence_quality"] = evidence_score
        
        if evidence_score < 0.5:
            warnings.append("Qualidade das evid√™ncias √© baixa")
        
        # 5. Detecta conflitos
        has_conflict = self._detect_conflicts(answer)
        if has_conflict:
            warnings.append("Poss√≠vel conflito normativo detectado")
            return {
                "is_valid": True,
                "confidence": ConfidenceLevel.BAIXA,
                "warnings": warnings,
                "scores": scores
            }
        
        # 6. Calcula confian√ßa geral
        confidence = self._calculate_confidence(scores, len(evidences))
        
        return {
            "is_valid": True,
            "confidence": confidence,
            "warnings": warnings if warnings else None,
            "scores": scores
        }
    
    def _is_no_answer(self, answer: str) -> bool:
        """Verifica se √© uma resposta de n√£o localiza√ß√£o"""
        answer_lower = answer.lower()
        
        for pattern in self.no_answer_patterns:
            if re.search(pattern, answer_lower):
                return True
        
        return False
    
    def _detect_conflicts(self, answer: str) -> bool:
        """Detecta men√ß√µes a conflitos normativos"""
        answer_lower = answer.lower()
        
        for pattern in self.conflict_patterns:
            if re.search(pattern, answer_lower):
                return True
        
        return False
    
    def _validate_citations(self, answer: str, num_evidences: int) -> float:
        """
        Valida presen√ßa e qualidade das cita√ß√µes
        Retorna score entre 0 e 1
        """
        citations = re.findall(self.citation_pattern, answer)
        
        if not citations:
            return 0.0
        
        # Verifica se cita√ß√µes s√£o v√°lidas
        valid_citations = [
            int(c) for c in citations 
            if c.isdigit() and 1 <= int(c) <= num_evidences
        ]
        
        if not valid_citations:
            return 0.0
        
        # Score baseado na propor√ß√£o de evid√™ncias citadas
        unique_citations = len(set(valid_citations))
        citation_coverage = unique_citations / num_evidences if num_evidences > 0 else 0
        
        # Score baseado na frequ√™ncia de cita√ß√µes no texto
        citation_density = len(valid_citations) / max(len(answer.split()), 1)
        citation_density = min(citation_density * 100, 1.0)  # Normaliza
        
        # Combina m√©tricas
        score = (citation_coverage * 0.7) + (citation_density * 0.3)
        
        return min(score, 1.0)
    
    def _validate_completeness(self, answer: str, question: str) -> float:
        """
        Valida se a resposta parece completa
        Retorna score entre 0 e 1
        """
        # Tamanho m√≠nimo esperado
        min_length = 100
        ideal_length = 500
        
        answer_length = len(answer)
        
        if answer_length < min_length:
            return 0.3
        
        if answer_length >= ideal_length:
            length_score = 1.0
        else:
            length_score = answer_length / ideal_length
        
        # Verifica estrutura (par√°grafos, pontua√ß√£o)
        has_structure = bool(re.search(r'\.\s+[A-Z]', answer))
        structure_score = 1.0 if has_structure else 0.7
        
        # Combina m√©tricas
        score = (length_score * 0.6) + (structure_score * 0.4)
        
        return min(score, 1.0)
    
    def _validate_evidence_quality(
        self,
        evidences: List["Evidence"],
        avg_score: float
    ) -> float:
        """
        Valida qualidade das evid√™ncias
        Retorna score entre 0 e 1
        """
        if not evidences:
            return 0.0
        
        # Score baseado na quantidade de evid√™ncias
        num_evidences = len(evidences)
        quantity_score = min(num_evidences / 5, 1.0)  # Ideal: 5+ evid√™ncias
        
        # Score baseado na relev√¢ncia m√©dia
        relevance_score = min(avg_score, 1.0)
        
        # Score baseado na preced√™ncia (se dispon√≠vel)
        precedence_scores = [
            1.0 - (e.precedence / 100) 
            for e in evidences 
            if e.precedence is not None
        ]
        
        if precedence_scores:
            precedence_score = sum(precedence_scores) / len(precedence_scores)
        else:
            precedence_score = 0.5  # Neutro se n√£o h√° info de preced√™ncia
        
        # Combina m√©tricas
        score = (
            quantity_score * 0.3 +
            relevance_score * 0.5 +
            precedence_score * 0.2
        )
        
        return min(score, 1.0)
    
    def _calculate_confidence(
        self,
        scores: Dict[str, float],
        num_evidences: int
    ) -> "ConfidenceLevel":
        from src.services.answer_service import ConfidenceLevel
        """
        Calcula n√≠vel de confian√ßa geral baseado nos scores
        """
        # Calcula score m√©dio
        avg_score = sum(scores.values()) / len(scores) if scores else 0.0
        
        # Ajusta baseado no n√∫mero de evid√™ncias
        if num_evidences < 2:
            avg_score *= 0.7
        elif num_evidences >= 5:
            avg_score *= 1.1
        
        avg_score = min(avg_score, 1.0)
        
        # Mapeia para n√≠veis de confian√ßa
        if avg_score >= 0.8:
            return ConfidenceLevel.ALTA
        elif avg_score >= 0.6:
            return ConfidenceLevel.MEDIA
        elif avg_score >= 0.4:
            return ConfidenceLevel.BAIXA
        else:
            return ConfidenceLevel.INSUFICIENTE
    
    def validate_question(self, question: str) -> Dict[str, Any]:
        """
        Valida se uma pergunta √© adequada
        """
        errors = []
        warnings = []
        
        # Verifica tamanho
        if len(question) < 5:
            errors.append("Pergunta muito curta (m√≠nimo 5 caracteres)")
        
        if len(question) > 1000:
            errors.append("Pergunta muito longa (m√°ximo 1000 caracteres)")
        
        # Verifica se tem conte√∫do significativo
        if not re.search(r'[a-zA-Z]{3,}', question):
            errors.append("Pergunta n√£o cont√©m palavras significativas")
        
        # Verifica se parece uma pergunta
        question_indicators = ['?', 'qual', 'como', 'quando', 'onde', 'quem', 'por que', 'o que']
        has_question_indicator = any(
            indicator in question.lower() 
            for indicator in question_indicators
        )
        
        if not has_question_indicator:
            warnings.append("Texto n√£o parece ser uma pergunta")
        
        is_valid = len(errors) == 0
        
        return {
            "is_valid": is_valid,
            "errors": errors if errors else None,
            "warnings": warnings if warnings else None
        }


if __name__ == "__main__":
    # Teste do validador
    print("üß™ Testando validador...")
    
    validator = ResponseValidator()
    
    # Teste 1: Valida pergunta
    question = "Qual o prazo para renova√ß√£o?"
    q_validation = validator.validate_question(question)
    print(f"‚úÖ Pergunta v√°lida: {q_validation['is_valid']}")
    
    # Teste 2: Valida resposta
    answer = "O prazo √© de 30 dias [1]. Conforme estabelecido na resolu√ß√£o [2], o processo deve ser iniciado com anteced√™ncia."
    
    from src.services.answer_service import Evidence
    evidences = [
        Evidence("Resolu√ß√£o 123", 1, "Resolu√ß√£o", "Texto exemplo", 0.9),
        Evidence("Portaria 456", 2, "Portaria", "Texto exemplo", 0.8),
    ]
    
    r_validation = validator.validate_response(question, answer, evidences, 0.85)
    print(f"‚úÖ Resposta v√°lida: {r_validation['is_valid']}")
    print(f"   Confian√ßa: {r_validation['confidence']}")
    print(f"   Scores: {r_validation['scores']}")
