"""
Processador de Texto
Limpeza, normalizaÃ§Ã£o e prÃ©-processamento de texto extraÃ­do de documentos.
"""

import re
import unicodedata
from typing import List, Optional


class TextProcessor:
    """
    Processa e limpa texto extraÃ­do de PDFs
    """
    
    def __init__(self):
        # PadrÃµes de limpeza
        self.patterns = {
            # Remove mÃºltiplos espaÃ§os
            "multiple_spaces": re.compile(r'\s+'),
            
            # Remove quebras de linha desnecessÃ¡rias
            "line_breaks": re.compile(r'\n{3,}'),
            
            # Remove caracteres de controle
            "control_chars": re.compile(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]'),
            
            # Corrige hifenizaÃ§Ã£o de palavras quebradas
            "hyphenation": re.compile(r'(\w+)-\s*\n\s*(\w+)'),
            
            # Remove cabeÃ§alhos/rodapÃ©s comuns
            "header_footer": re.compile(r'^(PÃ¡gina \d+|Page \d+|\d+/\d+)$', re.MULTILINE),
        }
        
        # Mapeamento de correÃ§Ãµes comuns em OCR
        self.ocr_corrections = {
            "G": "t",  # Exemplo do cÃ³digo original
            "0": "O",  # Zero -> O maiÃºsculo em contextos especÃ­ficos
            "1": "l",  # Um -> L minÃºsculo em contextos especÃ­ficos
        }
    
    def clean_text(self, text: str, aggressive: bool = False) -> str:
        """
        Limpa e normaliza texto
        
        Args:
            text: Texto a ser limpo
            aggressive: Se True, aplica limpeza mais agressiva
        """
        if not text:
            return ""
        
        # 1. Normaliza unicode
        text = self._normalize_unicode(text)
        
        # 2. Remove caracteres de controle
        text = self.patterns["control_chars"].sub("", text)
        
        # 3. Corrige hifenizaÃ§Ã£o
        text = self.patterns["hyphenation"].sub(r'\1\2', text)
        
        # 4. Remove cabeÃ§alhos/rodapÃ©s
        text = self.patterns["header_footer"].sub("", text)
        
        # 5. Normaliza espaÃ§os em branco
        text = self._normalize_whitespace(text)
        
        # 6. CorreÃ§Ãµes especÃ­ficas de OCR (se agressivo)
        if aggressive:
            text = self._apply_ocr_corrections(text)
        
        # 7. Remove linhas vazias excessivas
        text = self.patterns["line_breaks"].sub("\n\n", text)
        
        return text.strip()
    
    def _normalize_unicode(self, text: str) -> str:
        """
        Normaliza caracteres unicode
        """
        # Normaliza para forma NFKC (compatibilidade)
        text = unicodedata.normalize('NFKC', text)
        
        # Remove marcas diacrÃ­ticas opcionalmente
        # (comentado por padrÃ£o para preservar acentuaÃ§Ã£o em portuguÃªs)
        # text = ''.join(c for c in unicodedata.normalize('NFD', text)
        #                if unicodedata.category(c) != 'Mn')
        
        return text
    
    def _normalize_whitespace(self, text: str) -> str:
        """
        Normaliza espaÃ§os em branco
        """
        # Substitui mÃºltiplos espaÃ§os por um Ãºnico
        text = self.patterns["multiple_spaces"].sub(" ", text)
        
        # Normaliza quebras de linha
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        
        # Remove espaÃ§os no inÃ­cio/fim de cada linha
        lines = [line.strip() for line in text.split("\n")]
        text = "\n".join(lines)
        
        return text
    
    def _apply_ocr_corrections(self, text: str) -> str:
        """
        Aplica correÃ§Ãµes comuns de erros de OCR
        """
        # Aplica correÃ§Ãµes do dicionÃ¡rio
        for wrong, correct in self.ocr_corrections.items():
            # Aplica apenas em contextos especÃ­ficos para evitar substituiÃ§Ãµes incorretas
            # Exemplo: "G" -> "t" apenas em meio de palavra
            pattern = re.compile(rf'\b\w*{re.escape(wrong)}\w*\b')
            # Esta Ã© uma implementaÃ§Ã£o simplificada
            # Em produÃ§Ã£o, seria necessÃ¡rio contexto mais sofisticado
        
        return text
    
    def extract_sections(self, text: str) -> List[dict]:
        """
        Extrai seÃ§Ãµes do texto baseado em tÃ­tulos/numeraÃ§Ã£o
        """
        sections = []
        
        # PadrÃµes de tÃ­tulos comuns
        title_patterns = [
            r'^(CAPÃTULO|SEÃ‡ÃƒO|ARTIGO|Art\.?)\s+(\d+)',
            r'^(\d+\.)+\s+[A-Z]',
            r'^[A-Z][A-Z\s]{10,}$',
        ]
        
        current_section = {
            "title": "IntroduÃ§Ã£o",
            "content": [],
            "level": 0
        }
        
        for line in text.split("\n"):
            is_title = False
            
            for pattern in title_patterns:
                if re.match(pattern, line.strip()):
                    # Salva seÃ§Ã£o anterior
                    if current_section["content"]:
                        current_section["content"] = "\n".join(current_section["content"])
                        sections.append(current_section.copy())
                    
                    # Inicia nova seÃ§Ã£o
                    current_section = {
                        "title": line.strip(),
                        "content": [],
                        "level": self._infer_section_level(line)
                    }
                    is_title = True
                    break
            
            if not is_title and line.strip():
                current_section["content"].append(line)
        
        # Adiciona Ãºltima seÃ§Ã£o
        if current_section["content"]:
            current_section["content"] = "\n".join(current_section["content"])
            sections.append(current_section)
        
        return sections
    
    def _infer_section_level(self, title: str) -> int:
        """
        Infere nÃ­vel hierÃ¡rquico da seÃ§Ã£o
        """
        if re.match(r'^CAPÃTULO', title):
            return 1
        elif re.match(r'^SEÃ‡ÃƒO', title):
            return 2
        elif re.match(r'^(ARTIGO|Art\.?)', title):
            return 3
        elif re.match(r'^\d+\.', title):
            # Conta pontos para determinar nÃ­vel
            return title.count('.') + 1
        else:
            return 0
    
    def remove_tables(self, text: str) -> str:
        """
        Remove tabelas do texto (heurÃ­stica simples)
        """
        lines = text.split("\n")
        filtered_lines = []
        
        for line in lines:
            # HeurÃ­stica: linha com muitos espaÃ§os ou tabs pode ser tabela
            if line.count("  ") > 5 or line.count("\t") > 2:
                continue
            filtered_lines.append(line)
        
        return "\n".join(filtered_lines)
    
    def extract_metadata_from_text(self, text: str) -> dict:
        """
        Extrai metadados do texto (tÃ­tulo, data, etc)
        """
        metadata = {}
        
        # Extrai possÃ­vel tÃ­tulo (primeira linha em maiÃºsculas)
        lines = text.split("\n")
        for line in lines[:10]:  # Verifica primeiras 10 linhas
            if line.strip() and line.strip().isupper() and len(line.strip()) > 10:
                metadata["title"] = line.strip()
                break
        
        # Extrai datas
        date_pattern = r'\b(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})\b'
        dates = re.findall(date_pattern, text[:1000])  # Busca no inÃ­cio
        if dates:
            metadata["dates"] = dates
        
        # Extrai nÃºmeros de normativos
        normativo_pattern = r'\b(Lei|Decreto|ResoluÃ§Ã£o|Portaria)\s+n[ÂºÂ°]?\s*(\d+[./]?\d*)\b'
        normativos = re.findall(normativo_pattern, text[:2000], re.IGNORECASE)
        if normativos:
            metadata["normativos"] = [f"{tipo} {num}" for tipo, num in normativos]
        
        return metadata
    
    def split_into_sentences(self, text: str) -> List[str]:
        """
        Divide texto em sentenÃ§as
        """
        # PadrÃ£o simples de divisÃ£o por pontuaÃ§Ã£o
        sentence_pattern = r'[.!?]+\s+'
        sentences = re.split(sentence_pattern, text)
        
        # Limpa e filtra sentenÃ§as vazias
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def get_text_stats(self, text: str) -> dict:
        """
        Retorna estatÃ­sticas do texto
        """
        words = text.split()
        sentences = self.split_into_sentences(text)
        
        return {
            "total_chars": len(text),
            "total_words": len(words),
            "total_sentences": len(sentences),
            "avg_word_length": sum(len(w) for w in words) / len(words) if words else 0,
            "avg_sentence_length": len(words) / len(sentences) if sentences else 0,
        }


if __name__ == "__main__":
    # Teste do processador de texto
    print("ðŸ§ª Testando processador de texto...")
    
    processor = TextProcessor()
    
    # Teste 1: Limpeza bÃ¡sica
    dirty_text = """
    Este    Ã©  um   texto    com     espaÃ§os     irregulares.
    
    
    
    E quebras de linha excessivas.
    
    PÃ¡gina 1
    
    TambÃ©m tem cabeÃ§alhos.
    """
    
    clean = processor.clean_text(dirty_text)
    print(f"âœ… Texto limpo ({len(clean)} chars)")
    print(f"Original: {len(dirty_text)} chars")
    
    # Teste 2: EstatÃ­sticas
    stats = processor.get_text_stats(clean)
    print(f"ðŸ“Š Stats: {stats}")
    
    # Teste 3: ExtraÃ§Ã£o de metadados
    sample_text = """
    RESOLUÃ‡ÃƒO NÂº 5.956, DE 20 DE DEZEMBRO DE 2024
    
    DispÃµe sobre procedimentos de verificaÃ§Ã£o...
    """
    
    metadata = processor.extract_metadata_from_text(sample_text)
    print(f"ðŸ“‹ Metadados: {metadata}")
